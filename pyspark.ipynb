{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"A:/Zhao/bigdata/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Spark test\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines with a: 61, lines with b: 30\n"
     ]
    }
   ],
   "source": [
    "logFile = \"A:/Zhao/bigdata/spark/README.md\"\n",
    "logData = sc.textFile(logFile).cache()\n",
    "numAs = logData.filter(lambda s: 'a' in s).count()\n",
    "numBs = logData.filter(lambda s: 'b' in s).count()\n",
    "print (\"Lines with a: \"+ str(numAs) +\", lines with b: \"+ str(numBs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "\n",
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scala',\n",
       " 'java',\n",
       " 'hadoop',\n",
       " 'spark',\n",
       " 'akka',\n",
       " 'spark vs hadoop',\n",
       " 'pyspark',\n",
       " 'pyspark and spark']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x): print(x)\n",
    "words.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_filter = words.filter(lambda x: 'spark' in x)\n",
    "words_filter.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scala', 1),\n",
       " ('java', 1),\n",
       " ('hadoop', 1),\n",
       " ('spark', 1),\n",
       " ('akka', 1),\n",
       " ('spark vs hadoop', 1),\n",
       " ('pyspark', 1),\n",
       " ('pyspark and spark', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_map = words.map(lambda x: (x, 1))\n",
    "words_map.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce(f)\n",
    "\n",
    "from operator import add\n",
    "\n",
    "nums = sc.parallelize([1, 2, 3, 4, 5])\n",
    "nums.reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hadoop', (4, 5)), ('spark', (1, 2))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join(other, numPartitions = None)\n",
    "\n",
    "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
    "y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)])\n",
    "joined = x.join(y)\n",
    "joined.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cache()\n",
    "Persist this RDD with the default storage level (MEMORY_ONLY). \n",
    "You can also check if the RDD is cached or not.\n",
    "'''\n",
    "words.cache() \n",
    "words.persist().is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------\n",
    "**PySpark - Broadcast & Accumulator**\n",
    "\n",
    "For parallel processing, Apache Spark uses shared variables. \n",
    "A copy of shared variable goes on each node of the cluster when the driver sends a task to the executor on the cluster, \n",
    "so that it can be used for performing tasks.\n",
    "\n",
    "There are two types of shared variables supported by Apache Spark âˆ’\n",
    "**Broadcast & Accumulator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Broadcast**\n",
    "Broadcast variables are used to save the copy of data across all nodes. \n",
    "This variable is cached on all the machines and not sent on machines with tasks. \n",
    "A Broadcast variable has an attribute called value, which stores the data and is used to return a broadcasted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scala', 'java', 'hadoop', 'spark', 'akka']\n",
      "hadoop\n"
     ]
    }
   ],
   "source": [
    "words_new = sc.broadcast([\"scala\", \"java\", \"hadoop\", \"spark\", \"akka\"]) \n",
    "data = words_new.value \n",
    "print (data) \n",
    "elem = words_new.value[2] \n",
    "print (elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accumulator**\n",
    "Accumulator variables are used for aggregating the information through associative and commutative operations. \n",
    "For example, you can use an accumulator for a sum operation or counters (in MapReduce).\n",
    "An Accumulator variable has an attribute called value that is similar to what a broadcast variable has. \n",
    "It stores the data and is used to return the accumulator's value, but usable only in a driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "num = sc.accumulator(0) #initial value needed\n",
    "def f(x): \n",
    "   global num \n",
    "   num+=x \n",
    "rdd = sc.parallelize([10,20,30,40,50]) \n",
    "rdd.foreach(f) \n",
    "final = num.value \n",
    "print (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\hez\\\\AppData\\\\Local\\\\Temp\\\\spark-9a5bcb07-aa7b-45e2-8df2-c7e0747f485a\\\\userFiles-01a78d99-306b-4f4a-a636-1bd73f98709f\\\\logistic_regression.py'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SparkFiles\n",
    "\n",
    "from pyspark import SparkFiles\n",
    "finddistance = \"A:/Zhao/bigdata/spark/examples/src/main/python/mllib/logistic_regression.py\"\n",
    "finddistancename = \"logistic_regression.py\"\n",
    "sc.addFile(finddistance)\n",
    "SparkFiles.get(finddistancename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk Memory Serialized 2x Replicated\n"
     ]
    }
   ],
   "source": [
    "# StorageLevel\n",
    "\n",
    "import pyspark\n",
    "rdd1 = sc.parallelize([1,2])\n",
    "rdd1.persist( pyspark.StorageLevel.MEMORY_AND_DISK_2 )\n",
    "rdd1.getStorageLevel()\n",
    "print(rdd1.getStorageLevel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "**MLlib**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**test.data**\n",
    "1,1,5.0\n",
    "1,2,1.0\n",
    "1,3,5.0\n",
    "1,4,1.0\n",
    "2,1,5.0\n",
    "2,2,1.0\n",
    "2,3,5.0\n",
    "2,4,1.0\n",
    "3,1,1.0\n",
    "3,2,5.0\n",
    "3,3,1.0\n",
    "3,4,5.0\n",
    "4,1,1.0\n",
    "4,2,5.0\n",
    "4,3,1.0\n",
    "4,4,5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 5.145170763112815e-06\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "if __name__ == \"__main__\":\n",
    "       #sc = SparkContext(appName=\"Pspark mllib Example\")\n",
    "       data = sc.textFile(\"A:/Zhao/bigdata/test/test.data\")\n",
    "       ratings = data.map(lambda l: l.split(','))\\\n",
    "          .map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "   \n",
    "       # Build the recommendation model using Alternating Least Squares\n",
    "       rank = 10\n",
    "       numIterations = 10\n",
    "       model = ALS.train(ratings, rank, numIterations)\n",
    "   \n",
    "       # Evaluate the model on training data\n",
    "       testdata = ratings.map(lambda p: (p[0], p[1]))\n",
    "       predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "       ratesAndPreds = ratings.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "       MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "       print(\"Mean Squared Error = \" + str(MSE))\n",
    "   \n",
    "       # Save and load model\n",
    "       model.save(sc, \"A:/Zhao/bigdata/test/myCollaborativeFilter\")\n",
    "       sameModel = MatrixFactorizationModel.load(sc, \"A:/Zhao/bigdata/test/myCollaborativeFilter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
