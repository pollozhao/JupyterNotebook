{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"A:/Zhao/bigdata/spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Build the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Linear Regression Model\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd1 = sc.parallelize([('a',7),('a',2),('b',2)])\n",
    "rdd1 = spark.sparkContext.parallelize([('a',7),('a',2),('b',2)])\n",
    "rdd2 = spark.sparkContext.parallelize([(\"a\",[\"x\",\"y\",\"z\"]), (\"b\",[\"p\", \"r\"])])\n",
    "rdd3 = spark.sparkContext.parallelize(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 7, 'a', 2, 'b', 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.reduce(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.flatMapValues(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data\n",
    "rdd = sc.textFile('./houses/cal_housing.data')\n",
    "\n",
    "# Load in the header\n",
    "header = sc.textFile('./houses/cal_housing.domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['median_house_value,median_income,housing_median_age,total_rooms,total_bedrooms,population,households,latitude,longitude']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.5260000000000000e+005,8.3252000000000006e+000,4.1000000000000000e+001,8.8000000000000000e+002,1.2900000000000000e+002,3.2200000000000000e+002,1.2600000000000000e+002,3.7880000000000003e+001,-1.2223000000000000e+002',\n",
       " '3.5850000000000000e+005,8.3013999999999992e+000,2.1000000000000000e+001,7.0990000000000000e+003,1.1060000000000000e+003,2.4010000000000000e+003,1.1380000000000000e+003,3.7859999999999999e+001,-1.2222000000000000e+002']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['4.5260000000000000e+005',\n",
       "  '8.3252000000000006e+000',\n",
       "  '4.1000000000000000e+001',\n",
       "  '8.8000000000000000e+002',\n",
       "  '1.2900000000000000e+002',\n",
       "  '3.2200000000000000e+002',\n",
       "  '1.2600000000000000e+002',\n",
       "  '3.7880000000000003e+001',\n",
       "  '-1.2223000000000000e+002'],\n",
       " ['3.5850000000000000e+005',\n",
       "  '8.3013999999999992e+000',\n",
       "  '2.1000000000000000e+001',\n",
       "  '7.0990000000000000e+003',\n",
       "  '1.1060000000000000e+003',\n",
       "  '2.4010000000000000e+003',\n",
       "  '1.1380000000000000e+003',\n",
       "  '3.7859999999999999e+001',\n",
       "  '-1.2222000000000000e+002']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split lines on commas\n",
    "rdd = rdd.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Inspect the first 2 lines \n",
    "rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Inspect the first line \\nrdd.first()\\n\\n# Take top elements\\nrdd.top(2)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Inspect the first line \n",
    "rdd.first()\n",
    "\n",
    "# Take top elements\n",
    "rdd.top(2)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules \n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Map the RDD to a DF\n",
    "df = rdd.map(lambda line: Row(longitude=line[8], \n",
    "                              latitude=line[7], \n",
    "                              housingMedianAge=line[2],\n",
    "                              totalRooms=line[3],\n",
    "                              totalBedRooms=line[4],\n",
    "                              population=line[5], \n",
    "                              households=line[6],\n",
    "                              medianIncome=line[1],\n",
    "                              medianHouseValue=line[0])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|          households|    housingMedianAge|            latitude|           longitude|    medianHouseValue|        medianIncome|          population|       totalBedRooms|          totalRooms|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|1.260000000000000...|4.100000000000000...|3.788000000000000...|-1.22230000000000...|4.526000000000000...|8.325200000000000...|3.220000000000000...|1.290000000000000...|8.800000000000000...|\n",
      "|1.138000000000000...|2.100000000000000...|3.785999999999999...|-1.22220000000000...|3.585000000000000...|8.301399999999999...|2.401000000000000...|1.106000000000000...|7.099000000000000...|\n",
      "|1.770000000000000...|5.200000000000000...|3.785000000000000...|-1.22239999999999...|3.521000000000000...|7.257399999999999...|4.960000000000000...|1.900000000000000...|1.467000000000000...|\n",
      "|2.190000000000000...|5.200000000000000...|3.785000000000000...|-1.22250000000000...|3.413000000000000...|5.643099999999999...|5.580000000000000...|2.350000000000000...|1.274000000000000...|\n",
      "|2.590000000000000...|5.200000000000000...|3.785000000000000...|-1.22250000000000...|3.422000000000000...|3.846200000000000...|5.650000000000000...|2.800000000000000...|1.627000000000000...|\n",
      "|1.930000000000000...|5.200000000000000...|3.785000000000000...|-1.22250000000000...|2.697000000000000...|4.036800000000000...|4.130000000000000...|2.130000000000000...|9.190000000000000...|\n",
      "|5.140000000000000...|5.200000000000000...|3.784000000000000...|-1.22250000000000...|2.992000000000000...|3.659100000000000...|1.094000000000000...|4.890000000000000...|2.535000000000000...|\n",
      "|6.470000000000000...|5.200000000000000...|3.784000000000000...|-1.22250000000000...|2.414000000000000...|3.120000000000000...|1.157000000000000...|6.870000000000000...|3.104000000000000...|\n",
      "|5.950000000000000...|4.200000000000000...|3.784000000000000...|-1.22260000000000...|2.267000000000000...|2.080400000000000...|1.206000000000000...|6.650000000000000...|2.555000000000000...|\n",
      "|7.140000000000000...|5.200000000000000...|3.784000000000000...|-1.22250000000000...|2.611000000000000...|3.691199999999999...|1.551000000000000...|7.070000000000000...|3.549000000000000...|\n",
      "|4.020000000000000...|5.200000000000000...|3.785000000000000...|-1.22260000000000...|2.815000000000000...|3.203100000000000...|9.100000000000000...|4.340000000000000...|2.202000000000000...|\n",
      "|7.340000000000000...|5.200000000000000...|3.785000000000000...|-1.22260000000000...|2.418000000000000...|3.270500000000000...|1.504000000000000...|7.520000000000000...|3.503000000000000...|\n",
      "|4.680000000000000...|5.200000000000000...|3.785000000000000...|-1.22260000000000...|2.135000000000000...|3.075000000000000...|1.098000000000000...|4.740000000000000...|2.491000000000000...|\n",
      "|1.740000000000000...|5.200000000000000...|3.784000000000000...|-1.22260000000000...|1.913000000000000...|2.673600000000000...|3.450000000000000...|1.910000000000000...|6.960000000000000...|\n",
      "|6.200000000000000...|5.200000000000000...|3.785000000000000...|-1.22260000000000...|1.592000000000000...|1.916700000000000...|1.212000000000000...|6.260000000000000...|2.643000000000000...|\n",
      "|2.640000000000000...|5.000000000000000...|3.785000000000000...|-1.22260000000000...|1.400000000000000...|2.125000000000000...|6.970000000000000...|2.830000000000000...|1.120000000000000...|\n",
      "|3.310000000000000...|5.200000000000000...|3.785000000000000...|-1.22270000000000...|1.525000000000000...|2.774999999999999...|7.930000000000000...|3.470000000000000...|1.966000000000000...|\n",
      "|3.030000000000000...|5.200000000000000...|3.785000000000000...|-1.22270000000000...|1.555000000000000...|2.120200000000000...|6.480000000000000...|2.930000000000000...|1.228000000000000...|\n",
      "|4.190000000000000...|5.000000000000000...|3.784000000000000...|-1.22260000000000...|1.587000000000000...|1.991100000000000...|9.900000000000000...|4.550000000000000...|2.239000000000000...|\n",
      "|2.750000000000000...|5.200000000000000...|3.784000000000000...|-1.22270000000000...|1.629000000000000...|2.603299999999999...|6.900000000000000...|2.980000000000000...|1.503000000000000...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the top 20 rows \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- households: string (nullable = true)\n",
      " |-- housingMedianAge: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- medianHouseValue: string (nullable = true)\n",
      " |-- medianIncome: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- totalBedRooms: string (nullable = true)\n",
      " |-- totalRooms: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the data types of all `df` columns\n",
    "# df.dtypes\n",
    "\n",
    "# Print the schema of `df`\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all from `sql.types`\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Write a custom function to convert the data type of DataFrame columns\n",
    "def convertColumn(df, names, newType):\n",
    "  for name in names: \n",
    "     df = df.withColumn(name, df[name].cast(newType))\n",
    "  return df \n",
    "\n",
    "# Assign all column names to `columns`\n",
    "columns = ['medianHouseValue', 'medianIncome', 'housingMedianAge', 'totalBedRooms', 'totalRooms', 'population', 'households', 'latitude', 'longitude',  ]\n",
    "\n",
    "# Conver the `df` columns to `FloatType()`\n",
    "df = convertColumn(df, columns, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- households: float (nullable = true)\n",
      " |-- housingMedianAge: float (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- medianHouseValue: float (nullable = true)\n",
      " |-- medianIncome: float (nullable = true)\n",
      " |-- population: float (nullable = true)\n",
      " |-- totalBedRooms: float (nullable = true)\n",
      " |-- totalRooms: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|population|totalBedRooms|\n",
      "+----------+-------------+\n",
      "|     322.0|        129.0|\n",
      "|    2401.0|       1106.0|\n",
      "|     496.0|        190.0|\n",
      "|     558.0|        235.0|\n",
      "|     565.0|        280.0|\n",
      "|     413.0|        213.0|\n",
      "|    1094.0|        489.0|\n",
      "|    1157.0|        687.0|\n",
      "|    1206.0|        665.0|\n",
      "|    1551.0|        707.0|\n",
      "+----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('population','totalBedRooms').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|housingMedianAge|count|\n",
      "+----------------+-----+\n",
      "|            52.0| 1273|\n",
      "|            51.0|   48|\n",
      "|            50.0|  136|\n",
      "|            49.0|  134|\n",
      "|            48.0|  177|\n",
      "|            47.0|  198|\n",
      "|            46.0|  245|\n",
      "|            45.0|  294|\n",
      "|            44.0|  356|\n",
      "|            43.0|  353|\n",
      "|            42.0|  368|\n",
      "|            41.0|  296|\n",
      "|            40.0|  304|\n",
      "|            39.0|  369|\n",
      "|            38.0|  394|\n",
      "|            37.0|  537|\n",
      "|            36.0|  862|\n",
      "|            35.0|  824|\n",
      "|            34.0|  689|\n",
      "|            33.0|  615|\n",
      "+----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"housingMedianAge\").count().sort(\"housingMedianAge\",ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+-------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|summary|       households|  housingMedianAge|         latitude|          longitude|  medianHouseValue|      medianIncome|        population|    totalBedRooms|        totalRooms|\n",
      "+-------+-----------------+------------------+-----------------+-------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "|  count|            20640|             20640|            20640|              20640|             20640|             20640|             20640|            20640|             20640|\n",
      "|   mean|499.5396802325581|28.639486434108527|35.63186143109965|-119.56970444871473|206855.81690891474|3.8706710030346416|1425.4767441860465|537.8980135658915|2635.7630813953488|\n",
      "| stddev|382.3297528316098| 12.58555761211163|2.135952380602968|  2.003531742932898|115395.61587441359|1.8998217183639696|  1132.46212176534| 421.247905943133|2181.6152515827944|\n",
      "|    min|              1.0|               1.0|            32.54|            -124.35|           14999.0|            0.4999|               3.0|              1.0|               2.0|\n",
      "|    max|           6082.0|              52.0|            41.95|            -114.31|          500001.0|           15.0001|           35682.0|           6445.0|           39320.0|\n",
      "+-------+-----------------+------------------+-----------------+-------------------+------------------+------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0),\n",
       " Row(households=1138.0, housingMedianAge=21.0, latitude=37.86000061035156, longitude=-122.22000122070312, medianHouseValue=3.585, medianIncome=8.301400184631348, population=2401.0, totalBedRooms=1106.0, totalRooms=7099.0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all from `sql.functions` \n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Adjust the values of `medianHouseValue`\n",
    "df = df.withColumn(\"medianHouseValue\", col(\"medianHouseValue\")/100000)\n",
    "\n",
    "# Show the first 2 lines of `df`\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(households=126.0, housingMedianAge=41.0, latitude=37.880001068115234, longitude=-122.2300033569336, medianHouseValue=4.526, medianIncome=8.325200080871582, population=322.0, totalBedRooms=129.0, totalRooms=880.0, roomsPerHousehold=6.984126984126984, populationPerHousehold=2.5555555555555554, bedroomsPerRoom=0.14659090909090908)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all from `sql.functions` if you haven't yet\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Divide `totalRooms` by `households`\n",
    "roomsPerHousehold = df.select(col(\"totalRooms\")/col(\"households\"))\n",
    "\n",
    "# Divide `population` by `households`\n",
    "populationPerHousehold = df.select(col(\"population\")/col(\"households\"))\n",
    "\n",
    "# Divide `totalBedRooms` by `totalRooms`\n",
    "bedroomsPerRoom = df.select(col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "\n",
    "# Add the new columns to `df`\n",
    "df = df.withColumn(\"roomsPerHousehold\", col(\"totalRooms\")/col(\"households\")) \\\n",
    "   .withColumn(\"populationPerHousehold\", col(\"population\")/col(\"households\")) \\\n",
    "   .withColumn(\"bedroomsPerRoom\", col(\"totalBedRooms\")/col(\"totalRooms\"))\n",
    "   \n",
    "# Inspect the result\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-order and select columns\n",
    "df = df.select(\"medianHouseValue\", \n",
    "              \"totalBedRooms\", \n",
    "              \"population\", \n",
    "              \"households\", \n",
    "              \"medianIncome\", \n",
    "              \"roomsPerHousehold\", \n",
    "              \"populationPerHousehold\", \n",
    "              \"bedroomsPerRoom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `DenseVector`\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Define the `input_data` \n",
    "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Replace `df` with the new DataFrame\n",
    "df = spark.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can finally scale the data. You can use Spark ML to do this: this library will make machine learning on big data scalable and easy. You’ll find tools such as ML algorithms and everything you need to build practical ML pipelines. In this case, you don’t need to do that much preprocessing so a pipeline would maybe be overkill, but if you want to look into it, definitely consider visiting the this page:\n",
    "https://spark.apache.org/docs/latest/ml-pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=4.526, features=DenseVector([129.0, 322.0, 126.0, 8.3252, 6.9841, 2.5556, 0.1466]), features_scaled=DenseVector([0.3062, 0.2843, 0.3296, 4.3821, 2.8228, 0.2461, 2.5264])),\n",
       " Row(label=3.585, features=DenseVector([1106.0, 2401.0, 1138.0, 8.3014, 6.2381, 2.1098, 0.1558]), features_scaled=DenseVector([2.6255, 2.1202, 2.9765, 4.3696, 2.5213, 0.2031, 2.6851]))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import `StandardScaler` \n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Initialize the `standardScaler`\n",
    "standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "# Fit the DataFrame to the scaler\n",
    "scaler = standardScaler.fit(df)\n",
    "\n",
    "# Transform the data in `df` with the scaler\n",
    "scaled_df = scaler.transform(df)\n",
    "\n",
    "# Inspect the result\n",
    "scaled_df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------\n",
    "**Building A Machine Learning Model With Spark ML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = scaled_df.randomSplit([.8,.2],seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `LinearRegression`\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Initialize `lr`\n",
    "lr = LinearRegression(labelCol=\"label\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the data to the model\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.1340115638008952, 0.14999),\n",
       " (1.4485018834650096, 0.14999),\n",
       " (1.5713396046425587, 0.14999),\n",
       " (1.7496542762527307, 0.283),\n",
       " (1.2438468929500472, 0.366)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "predicted = linearModel.transform(test_data)\n",
    "\n",
    "# Extract the predictions and the \"known\" correct labels\n",
    "predictions = predicted.select(\"prediction\").rdd.map(lambda x: x[0])\n",
    "labels = predicted.select(\"label\").rdd.map(lambda x: x[0])\n",
    "\n",
    "# Zip `predictions` and `labels` into a list\n",
    "predictionAndLabel = predictions.zip(labels).collect()\n",
    "\n",
    "# Print out first 5 instances of `predictionAndLabel` \n",
    "predictionAndLabel[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "**Evaluating the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,0.0,0.0,0.2796215289269825,0.0,0.0,0.0]\n",
      "0.9841344205626824\n"
     ]
    }
   ],
   "source": [
    "# Coefficients for the model\n",
    "print(linearModel.coefficients)\n",
    "\n",
    "# Intercept for the model\n",
    "print(linearModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8765335684459216\n",
      "0.42282227755911483\n"
     ]
    }
   ],
   "source": [
    "# Get the RMSE\n",
    "print(linearModel.summary.rootMeanSquaredError)\n",
    "\n",
    "# Get the R2\n",
    "print(linearModel.summary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
